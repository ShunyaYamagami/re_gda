{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import normalized_mutual_info_score as NMI\n",
    "\n",
    "from z_common import config_to_execute\n",
    "from clustering import run_clustering\n",
    "from dataset import get_datasets\n",
    "from simclr import SimCLR\n",
    "from log_functions import *\n",
    "\n",
    "from models.baseline_encoder import Encoder\n",
    "from models.alexnet_simclr import AlexSimCLR\n",
    "from models.resnet_simclr import ResNetSimCLR\n",
    "from loss.nt_xent import NTXentLoss\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "from glob import glob\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# titles = ['dslr_webcam', 'amazon_dslr', 'webcam_amazon']\n",
    "# file_domains = ['dw', 'ad', 'wa']\n",
    "# log_files = ['../log_dw.txt', '../log_ad.txt', '../log_wa.txt']\n",
    "\n",
    "EXEC_REP_NUM = 2\n",
    "CLUST_REP_NUM = 3\n",
    "title = 'svhn_synth_mnist_mnistm'\n",
    "file_domain = 'svhn_synth_mnist_mnistm'\n",
    "log_file = f'./record/Digit/{title}_logs.txt'\n",
    "prompts_files_t = [glob(f\"./record/Digit/CUDA{i}/svhn_synth_mnist_mnistm_*/prompts.log\") for i in range(10)]\n",
    "\n",
    "# 各ディレクトリのprompts.logをcuda_dir毎に取得し, 2次元リストを作成\n",
    "prompts_files = []\n",
    "for cudaf in prompts_files_t:\n",
    "    if len(cudaf) != 0:\n",
    "        prompts_files.append(cudaf)\n",
    "\n",
    "\n",
    "\"\"\" 各ファイルのprompts.logを1つのログファイルを作成 \"\"\"\n",
    "output_texts = []\n",
    "for cuda_porompts_files in prompts_files:\n",
    "    log_texts = []\n",
    "    for prompts in cuda_porompts_files:\n",
    "        with open(prompts, 'r') as f:\n",
    "            logs = f.read()\n",
    "        split_texts = logs.split('\\n')\n",
    "        split_texts = [f\"{ft}\\n\" for ft in split_texts]\n",
    "        title_text = split_texts[:14]\n",
    "\n",
    "        log_text = [\n",
    "            line for line in split_texts \n",
    "            if 'Epoch:' in line\n",
    "            or 'nmi:' in line\n",
    "            or 'nmi class:' in line\n",
    "            or 'domain_accuracy:' in line\n",
    "        ]\n",
    "        log_text.insert(0, '\\n==========================================\\n')\n",
    "        log_texts.append(log_text)\n",
    "\n",
    "    output_text = sum(log_texts, [])\n",
    "    output_texts.append(np.concatenate([title_text, output_text]))\n",
    "\n",
    "# ログファイル書き込み\n",
    "output_texts = np.concatenate(output_texts)\n",
    "with open(log_file, 'w', newline='\\n') as f:\n",
    "    f.writelines(output_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    dft: ログから得た値(縦持ち)\n",
    "    df: ログから得た値(横持ち). 各実行,各クラスタリングそれぞれの値を全て保持.\n",
    "            レコード数: len(aug_ilst) * EXEC_REP_NUM * CLUST_REP_NUM\n",
    "    dfg: クラスタリングの平均値をまとめた.\n",
    "            レコード数: len(aug_ilst) * EXEC_REP_NUM\n",
    "    dfg_avg: その平均値をまとめた\n",
    "            レコード数: len(aug_ilst)\n",
    "    csvに書き込んでいく.\n",
    "\"\"\"\n",
    "dfs = {}\n",
    "dfgs = {}\n",
    "dfg_avgs = {}\n",
    "\n",
    "\n",
    "\"\"\" ログファイルからnmi等を記した行のみを取得し, DataFrameを作る. \"\"\"\n",
    "with open(log_file, 'r') as f:\n",
    "    text = f.read()\n",
    "split_text = text.split('\\n')\n",
    "log_text = [line for line in split_text if 'nmi:' in line or 'nmi class:' in line or 'domain_accuracy:' in line]\n",
    "split_logs = np.array([line.split(':') for line in log_text])\n",
    "\n",
    "dft = pd.DataFrame(split_logs, columns=['title', 'result'])\n",
    "\n",
    "augs_list = [\n",
    "    'none',\n",
    "    'jigsaw_const_phase',\n",
    "    'jigsaw',\n",
    "    # 'mask',\n",
    "    # 'jigsaw_mask',\n",
    "    # 'mask_const_phase',\n",
    "    'jigsaw_mask_const_phase',\n",
    "]\n",
    "augs_rep = sum([[aug for _ in range(EXEC_REP_NUM * CLUST_REP_NUM)] for aug in augs_list], [])\n",
    "df = pd.DataFrame(augs_rep, columns=['augs'])\n",
    "df['exec_number'] = sum([[i//EXEC_REP_NUM for i in range(EXEC_REP_NUM * CLUST_REP_NUM)] for _ in range(len(augs_list))], [])\n",
    "df['nmi_domain'] = dft[dft['title']=='nmi'].result.values\n",
    "df['nmi_class'] = dft[dft['title']=='nmi class'].result.values\n",
    "df['domain_accuracy'] = dft[dft['title']=='domain_accuracy'].result.values\n",
    "df[['nmi_domain', 'nmi_class', 'domain_accuracy']] = df[['nmi_domain', 'nmi_class', 'domain_accuracy']].astype('float').round(5)\n",
    "\n",
    "dfg = df.groupby(['augs', 'exec_number']).mean().reset_index()\n",
    "dfg_avg = dfg.drop('exec_number', axis=1).groupby('augs').mean().reset_index()\n",
    "\n",
    "\n",
    "\"\"\" csv書き込み \"\"\"\n",
    "df.to_csv(f'./record/Digit/{title}_result_df.csv', header=True, index=False)\n",
    "dfg.to_csv(f'./record/Digit/{title}_result_dfg.csv', header=True, index=False)\n",
    "dfg_avg.to_csv(f'./record/Digit/{title}_result_dfg_avg.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('generic')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "910681d38576edcdf845dadbeb8ce131b44b32e4da67ad4b7e8bb1a3790e2184"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
